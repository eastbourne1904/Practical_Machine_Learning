---
title: "Practical Machine Learning - Course Project"
author: "Alasdair Hazen"
output: html_document
---

## Executive Summary  
One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, the goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants and predict the manner in which they did the exercise. To do this multiple models, such as randomForest, ntree, rt, cv, gbm,  were built and compared to maximize speed and accuracy.  Model "randomForest" was chosen as a final method.  

## Question  

Using data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. Participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The question we are exploring is: can we predict the activity (class A-E) participant is most likely to perform?  

* R scripts have been produced and testing using Windows 7 and Windows 10 with R version 3.2.2 (2015-08-14),platform x86_64-w64-mingw32  

## Imput Data  
Check to see if data files exist. If they do not exist, download from website: "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"  

```{r input_data, echo=FALSE}
# Load data. Import if necessary
if (!file.exists("pml-training.csv")) {
  download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", 
                destfile = "pml-training.csv")
}
if (!file.exists("pml-testing.csv")) {
  download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", 
                destfile = "pml-testing.csv")
}

# Convert NA,NULL,#DIV/0! when reading in the data
pmlTrainingSet <- read.csv("pml-training.csv", header = TRUE, sep = ",",na.strings=c('NA','','#DIV/0!'))
```

Check libraries. If libraries are not loaded, load them.  

- Library List Used

    - caret
    - randomForest
    - foreach  
    - doParallel  
    - rpart
    - rattle  

```{r check_libraries,echo=FALSE, warning=FALSE}
# Check to see if all the packages that are needed are installed and loaded.
# If they aren't installed and/or loaded, load them. As it is not part of the overall model, code will not be revealed here.
require(caret)
require(randomForest)
require(foreach)
require(doParallel)
require(rpart)
require(rattle)

pkgTest <- function(x)
{
  if (!require(x,character.only = TRUE))
  {
    install.packages(x,dependencies = TRUE)
       if(!require(x,character.only = TRUE)) stop("Package not found")
  }
}

library(caret)
library(randomForest)
library(foreach)
library(doParallel)
library(rpart)
library(rattle)

set.seed(1979)
```

## Features/Attributes  
We are only going to choose features on columns that are meaningful. That is to say, we only want numerical data. There are two ways to do this. We can either explicity program what columns we want, via grepl, or we can remove the first 8 columns and the classe column. Visually we can see that these do not contain numeric data.  I chose to remove these columns.  

```{r}
# Convert columns 8 to end minus the classe column to numeric
for(i in c(8:ncol(pmlTrainingSet)-1)){
  pmlTrainingSet[,i]=as.numeric(as.character((pmlTrainingSet[,i])))}

pmlTraining_dataSet <- colnames(pmlTrainingSet[colSums(is.na(pmlTrainingSet)) == 0])[-(1:7)]

pmlTraining_ModelDataSet <- pmlTrainingSet[pmlTraining_dataSet]
pmlTraining_dataSet
```  

## Algorithm  

Now we want to split the data into 60% testing data, to build our model and 40% testing, to test our model.  

```{r split_data_into_training_and_testing_data}             
inTrain <- createDataPartition(y=pmlTrainingSet$classe, p=0.6, list=FALSE )
training <- pmlTraining_ModelDataSet[inTrain,]
testing <- pmlTraining_ModelDataSet[-inTrain,]
dim(training)
dim(testing)
```  

## Parameters  

Before we run through a model, we want to check for covariates that have virtually no variability and watch that it does not overfit.  

```{r near_zero_variance}
nsv <- nearZeroVar(training, saveMetrics = TRUE)
nsv
```  

Given the variables are FALSE, we don't need to preclude these from our model set.  

###Cross-Validation      

``````{r cross_validation}
# rpart
rpart_model <- train(classe ~ .,method="rpart",data=training)
print(rpart_model$finalModel)
fancyRpartPlot(rpart_model$finalModel)
```  

##Evaluation  

In building my randomForest model I tested the model with and without the out-of-bag error parameter (OOB). There was no difference in the outcome. This is evidence that it is as accurate as uing a test set of the same size as the training set, we remove the need to set aside a test set for our model.  

```{r compare_randomForest_with_outOfBagError}
# rf (randomForest)
fit <- randomForest(as.factor(classe) ~ ., data=training, importance=TRUE, ntree=2000)
varImpPlot(fit)
# predict with OOB (Out of bag error)
Prediction <- predict(fit, training,OOB=TRUE,type="response")
confusionMatrix(Prediction,training$classe)
```  

From our model we can see that we have a 99% accuracy rate.  And with using Out Of Bag error equal to TRUE we remove the need to set aside a test set for our model.   

```{r apply_prediction_to_test}
Prediction <- predict(fit, testing,OOB=TRUE,type="response")
confusionMatrix(Prediction,testing$classe)
```