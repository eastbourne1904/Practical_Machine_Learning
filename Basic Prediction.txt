> pmlTraining <- read.csv("pml-training.csv")
> inTrain <- createDataPartition(y=pmlTraining$classe,p=0.7,list=FALSE)
> training <- pmlTraining[inTrain,]
> testing <- pmlTraining[-inTrain,]
> dim(training)
[1] 13737   160
> dim(testing)
[1] 5885  160
> modFit <- train(classe ~ .,method="rpart",data=training)
> print(modFit$finalModel)
n= 271  # There are 271 nodes in the training set

node), split, n, loss, yval, (yprob)    # probability will be in split
      * denotes terminal node

1) root 271 197 A (0.27 0.19 0.18 0.17 0.2)  
  2) X< 5593 74   0 A (1 0 0 0 0) *   # if x < 5593 then probabiliy is that they will be in classe A
  3) X>=5593 197 144 E (0 0.26 0.24 0.23 0.27)  
    6) X< 16035.5 144  93 B (0 0.35 0.33 0.31 0) *
    7) X>=16035.5 53   0 E (0 0 0 0 1) *
> plot(modFit$finalModel,uniform=TRUE, main="Classification Tree")
> text(modFit$finalModel,use.n=TRUE,all=TRUE,cex=.8)
> library(rattle)
> fancyRpartPlot(modFit$finalModel)   # shows a fancier version of plain rpart plot
> predict(modFit,newdata=testing)    # predict new values
  [1] A A A A A A A A A A A A A A A A A A A A A A A A A A A A
 [29] A A A A A A A B B B B B B B B B B B B B B B B B B B B B
 [57] B B B B B B B B B B B B B B B B B B B B B B B B B B B B
 [85] B B B B B B B B B B B B B B B B B B B B B B B B B E E E
[113] E E E E E E E E E E E E E E E E E E E E E E E
Levels: A B C D E