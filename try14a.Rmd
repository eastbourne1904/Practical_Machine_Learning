--- 
title: "Practical Machine Learning - Course Project" 
author: "Alasdair Hazen" 
output: html_document 
--- 

## Executive Summary   

One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, the goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants and predict the manner in which they did the exercise. To do this multiple models, such as randomForest, ntree, rt, cv, gbm,  were built and compared to maximize speed and accuracy.  Model "randomForest" was chosen as a final method.   

## Question   

Using data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. Participants were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The question we are exploring is: can we predict the activity (class A-E) participant is most likely to perform?   

* R scripts have been produced and testing using Windows 7 and Windows 10 with R version 3.2.2 (2015-08-14),platform x86_64-w64-mingw32   

## Imput Data   

Check to see if data files exist. If they do not exist, download from website: "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"   


```{r input_data, echo=FALSE} 
# Download data.
#url_raw_training <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
#file_dest_training <- "pml-training.csv"
#download.file(url=url_raw_training, destfile=file_dest_training, method="curl")
#url_raw_testing <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
#file_dest_testing <- "pml-testing.csv"
#download.file(url=url_raw_testing, destfile=file_dest_testing, method="curl")

# Load data. Import if necessary 

if (!file.exists("pml-training.csv")) { 
  download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",  
                destfile = "pml-training.csv") 
} 
if (!file.exists("pml-testing.csv")) { 
  download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",  
                destfile = "pml-testing.csv") 
} 
```
Check libraries. If libraries are not loaded, load them.   

- Library List Used 
    - caret 
    - randomForest 
    - foreach   
    - doParallel   
    - rpart 
    - rattle   
 
```{r check_libraries,echo=FALSE, warning=FALSE} 
# Check to see if all the packages that are needed are installed and loaded. 
# If they aren't installed and/or loaded, load them. As it is not part of the overall model, code will not be revealed here. 

require(caret) 
require(randomForest) 
require(foreach) 
require(doParallel) 
require(rpart) 
require(rattle) 
 
pkgTest <- function(x) 
{ 
  if (!require(x,character.only = TRUE)) 
  { 
    install.packages(x,dependencies = TRUE) 
       if(!require(x,character.only = TRUE)) stop("Package not found") 
  } 
} 

library(caret) 
library(randomForest) 
library(foreach) 
library(doParallel) 
library(rpart) 
library(rattle) 
#library(AppliedPredictiveModeling)
 
set.seed(1979) 

# Convert NA,NULL,#DIV/0! when reading in the data 

pmlTrainingSet <- read.csv("pml-training.csv", header = TRUE, sep =  ",",na.strings=c('NA','','#DIV/0!')) 
pmlTestingSet <- read.csv("pml-testing.csv", header = TRUE, sep =  ",",na.strings=c('NA','','#DIV/0!'))

pmlTrainingSet.names <- colnames(pmlTrainingSet)
pmlTestingSet.names <- colnames(pmlTestingSet)
```

```{r remove_after_testing}
# Verify that the column names (excluding classe and problem_id) are identical in the training and test set.
#all.equal(colnames_train[1:length(colnames_train)-1], colnames_test[1:length(colnames_train)-1])
all.equal(pmlTrainingSet.names[1:length(pmlTrainingSet.names)-1], pmlTestingSet.names[1:length(pmlTrainingSet.names)-1])
```

We are only going to choose features on columns that are meaningful. That is to say, we only want numerical data. There are two ways to do this. We can either explicity program what columns we want, via grepl, or we can remove the first 8 columns and the classe column. Visually we can see that these do not contain numeric data.  I chose to remove these columns. 

```{r drop_NA_data}

# Count the number of values not equal to na (!is.na) in each column.
goodDataValues <- function(x) {
  as.vector(apply(x, 2, function(x) length(which(!is.na(x)))))
}

# Build vector of missing data or NA columns that got missed in our original inport.
workableData <- goodDataValues(pmlTrainingSet)
dropData <- c()
for (cnt in 1:length(workableData)) {
  if (workableData[cnt] < nrow(pmlTrainingSet)) {
    dropData <- c(dropData, pmlTrainingSet.names[cnt])
  }
}
```

```{r drop_unnecessary_columns}

# Drop first 7 columns as they're unnecessary for predicting.
pmlTrainingSet <- pmlTrainingSet[,!(names(pmlTrainingSet) %in% dropData)]
pmlTrainingSet <- pmlTrainingSet[,8:length(colnames(pmlTrainingSet))]

pmlTestingSet <- pmlTestingSet[,!(names(pmlTestingSet) %in% dropData)]
pmlTestingSet <- pmlTestingSet[,8:length(colnames(pmlTestingSet))]

# Show remaining columns.
colnames(pmlTrainingSet)
colnames(pmlTestingSet)
```

Before we run through a model, we want to check for covariates that have virtually no variability and watch that it does not overfit.   

```{r near_zero_variance} 
nzv <- nearZeroVar(pmlTrainingSet, saveMetrics=TRUE)
nzv
```  

Given the variables are FALSE, we don't need to preclude these from our model set.  

##Algorithm  

Now we want to split the data into 60% testing data, to build our model and 40% testing, to test our model.  

```{r split_data_into_training_and_testing_data}    
inTrain <- createDataPartition(y=pmlTrainingSet$classe, p=0.6, list=FALSE ) 
training <- pmlTrainingSet[inTrain,] 
testing <- pmlTrainingSet[-inTrain,] 
dim(training) 
dim(testing) 

``` 

## Parameters   

I attempted several cross validation models  
- Including, but not limited to
    - out-of-the-box classification tree 
    - randomForest 
    - class
    - rf  
The processing times on some made the using of them prohibative.  I will go through a few here.  

####Classification Tree:  

```{r}
set.seed(1979)
model1 <- train(training$classe ~ .,  preProcess=c("center", "scale"), data = training, method="rpart")
print(model1, digits=4)
```
```{r graph_model}
print(model1$finalModel,digits=4)
# graph
fancyRpartPlot(model1$finalModel)
```

Run against test data  

```{r}
prediction <- predict(model1, newdata=testing)
print(confusionMatrix(prediction, testing$classe), digits=4)
```  

####rpart with method cv:  

```{r}
set.seed(1979)
model2 <- train(training$classe ~ .,  trControl=trainControl(method = "cv", number = 4), data = training, method="rpart")
print(model2, digits=3)
```  

Accuracy was slightly less than model1.  

```{r}
set.seed(666)
model2 <- train(training$classe ~ .,  preProcess=c("center", "scale"), trControl=trainControl(method = "cv", number = 4), data = training, method="rpart")
print(model2, digits=3)
```  

Accuracy for model2 was exactly the same as model1.  

####Random Forest  

```{r}
set.seed(1979)
model3 <- train(training$classe ~ ., method="rf", trControl=trainControl(method = "cv", number = 4), data=training)
print(model3, digits=3)
```  

While this model was the slowest, it was also the most accurate.  

```{r predict_using_model3}
prediction <- predict(model3, newdata=testing)
print(confusionMatrix(prediction, testing$classe), digits=4)
```  

####Run prediction model against subset test data 

```{r run_model3_against_test}
print(predict(model3, newdata=testing))
```  

####Run prediction model against 20 test data  

